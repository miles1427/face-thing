<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Recognition App</title>
    <!-- Tailwind CSS for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Google Fonts for a modern look -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- face-api.js for face recognition -->
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

    <style>
        /* Custom styles for the app */
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Style for the video and canvas container */
        #videoContainer {
            position: relative;
            width: 100%;
            max-width: 720px;
            aspect-ratio: 16 / 9;
            border-radius: 0.75rem; /* rounded-xl */
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        /* Position the canvas directly on top of the video */
        #videoContainer canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        /* Custom styles for Gemini buttons */
        .gemini-btn {
            background-color: #4A5568; /* gray-700 */
            border: 1px solid #718096; /* gray-500 */
        }
        .gemini-btn:hover {
            background-color: #2D3748; /* gray-800 */
        }
        .gemini-btn:disabled {
            background-color: #2D3748; /* gray-800 */
            opacity: 0.5;
            cursor: not-allowed;
        }
    </style>
</head>
<body class="bg-gray-900 text-white flex items-center justify-center min-h-screen p-4">

    <div class="w-full max-w-4xl mx-auto flex flex-col items-center space-y-6">
        <!-- Header -->
        <div class="text-center">
            <h1 class="text-4xl md:text-5xl font-bold tracking-tight bg-gradient-to-r from-blue-400 to-purple-500 text-transparent bg-clip-text">
                Real-Time Face Recognition
            </h1>
            <p class="mt-2 text-lg text-gray-400">A demonstration of face detection and landmark recognition in the browser.</p>
        </div>

        <!-- Main App Card -->
        <div class="bg-gray-800 border border-gray-700 rounded-2xl p-6 md:p-8 w-full flex flex-col items-center space-y-4 shadow-2xl shadow-gray-900/50">
            <!-- Video and Canvas Container -->
            <div id="videoContainer" class="bg-gray-900 flex items-center justify-center">
                <video id="video" autoplay muted playsinline></video>
                <!-- This message is shown before the camera starts -->
                 <div id="placeholder" class="text-gray-500">
                    Your camera feed will appear here
                </div>
            </div>

            <!-- Controls and Status -->
            <div id="controls" class="w-full max-w-md flex flex-col items-center space-y-3">
                <button id="startButton" class="w-full bg-blue-600 hover:bg-blue-700 text-white font-semibold py-3 px-6 rounded-lg transition-all duration-300 ease-in-out transform hover:scale-105 shadow-lg">
                    Start Camera & Detect
                </button>
                <div class="w-full grid grid-cols-1 md:grid-cols-2 gap-3 pt-2">
                     <button id="analyzeButton" disabled class="w-full gemini-btn text-white font-semibold py-3 px-6 rounded-lg transition-all duration-300 ease-in-out transform hover:scale-105 shadow-lg">
                        ✨ Analyze My Expression
                    </button>
                     <button id="personaButton" disabled class="w-full gemini-btn text-white font-semibold py-3 px-6 rounded-lg transition-all duration-300 ease-in-out transform hover:scale-105 shadow-lg">
                        ✨ Describe a Persona
                    </button>
                </div>
                 <div id="loadingMessage" class="text-center text-gray-400 hidden h-6">
                    <span class="animate-pulse">Loading models, please wait...</span>
                </div>
            </div>
            
            <!-- Gemini API Output -->
            <div id="geminiOutput" class="w-full max-w-md bg-gray-900/50 border border-gray-700 rounded-lg p-4 mt-4 text-gray-300 hidden min-h-[100px] transition-opacity duration-500">
                <p id="geminiText"></p>
            </div>
        </div>
        
        <!-- Footer -->
        <p class="text-sm text-gray-600">Powered by face-api.js, Gemini & Tailwind CSS</p>
    </div>

    <script>
        const video = document.getElementById('video');
        const startButton = document.getElementById('startButton');
        const loadingMessage = document.getElementById('loadingMessage');
        const placeholder = document.getElementById('placeholder');
        const videoContainer = document.getElementById('videoContainer');
        const analyzeButton = document.getElementById('analyzeButton');
        const personaButton = document.getElementById('personaButton');
        const geminiOutput = document.getElementById('geminiOutput');
        const geminiText = document.getElementById('geminiText');

        let modelsLoaded = false;
        let stream = null;
        let currentExpression = null;

        // Function to load the face detection models
        async function loadModels() {
            // Reverted to the more reliable NPM CDN URL for models
            const MODEL_URL = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@0.22.2/weights';
            
            loadingMessage.classList.remove('hidden');
            startButton.disabled = true;

            try {
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
                    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
                    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
                    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
                ]);

                modelsLoaded = true;
                loadingMessage.textContent = 'Models loaded successfully!';
                setTimeout(() => loadingMessage.classList.add('hidden'), 2000);
                startButton.disabled = false;
                console.log("Models Loaded");
            } catch (error) {
                console.error('Failed to load models:', error);
                loadingMessage.textContent = 'Error: Could not load models. Please refresh.';
                loadingMessage.classList.remove('text-gray-400');
                loadingMessage.classList.add('text-red-500');
            }
        }

        // Gemini API Call Function
        async function callGeminiAPI(prompt, maxRetries = 3) {
            geminiOutput.classList.remove('hidden');
            geminiText.innerHTML = '<span class="animate-pulse">✨ Thinking...</span>';

            const apiKey = ""; // Canvas will provide this
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;
            const payload = { contents: [{ parts: [{ text: prompt }] }] };

            for (let i = 0; i < maxRetries; i++) {
                try {
                    const response = await fetch(apiUrl, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(payload)
                    });

                    if (!response.ok) {
                        throw new Error(`API request failed with status ${response.status}`);
                    }

                    const result = await response.json();
                    const text = result.candidates?.[0]?.content?.parts?.[0]?.text;

                    if (text) {
                        geminiText.innerText = text;
                        return; // Success
                    } else {
                         throw new Error('Invalid response from Gemini API.');
                    }
                } catch (error) {
                    console.error('Error calling Gemini API:', error);
                    if (i === maxRetries - 1) {
                         geminiText.innerText = 'Sorry, something went wrong. Please try again.';
                         break;
                    }
                    // Exponential backoff
                    const delay = Math.pow(2, i) * 1000;
                    console.log(`Retrying in ${delay}ms...`);
                    await new Promise(res => setTimeout(res, delay));
                }
            }
        }
        
        analyzeButton.addEventListener('click', () => {
            if (currentExpression) {
                const prompt = `Write a short, fun, and creative analysis of the facial expression: "${currentExpression}". Keep it under 50 words.`;
                callGeminiAPI(prompt);
            }
        });
        
        personaButton.addEventListener('click', () => {
             if (currentExpression) {
                const prompt = `Based on a person showing a "${currentExpression}" facial expression, invent a fun, one-sentence character persona for them. For example, for "happy", you could say "A joyful adventurer who just discovered a hidden treasure."`;
                callGeminiAPI(prompt);
            }
        });

        // Function to start the webcam
        async function startVideo() {
            if (!modelsLoaded) { return; }
            try {
                stream = await navigator.mediaDevices.getUserMedia({ video: {} });
                video.srcObject = stream;
                placeholder.classList.add('hidden');
                startButton.textContent = 'Stop Camera';
            } catch (err) {
                console.error('Error accessing webcam:', err);
                // Replaced alert with a UI message
                loadingMessage.classList.remove('hidden');
                loadingMessage.textContent = 'Error: Could not access webcam. Please check permissions.';
                loadingMessage.classList.remove('text-gray-400');
                loadingMessage.classList.add('text-red-500');
            }
        }
        
        // Function to stop the webcam
        function stopVideo() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                video.srcObject = null;
                placeholder.classList.remove('hidden');
                startButton.textContent = 'Start Camera & Detect';
                analyzeButton.disabled = true;
                personaButton.disabled = true;
                currentExpression = null;
                geminiOutput.classList.add('hidden');
                
                const existingCanvas = videoContainer.querySelector('canvas');
                if (existingCanvas) {
                    videoContainer.removeChild(existingCanvas);
                }
            }
        }

        startButton.addEventListener('click', () => {
             if (video.srcObject) {
                stopVideo();
            } else {
                startVideo();
            }
        });
        
        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video);
            videoContainer.appendChild(canvas);
            
            const displaySize = { width: video.clientWidth, height: video.clientHeight };
            faceapi.matchDimensions(canvas, displaySize);

            const detectionInterval = setInterval(async () => {
                if(video.paused || video.ended || !video.srcObject) {
                    clearInterval(detectionInterval);
                    return;
                }
                
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceExpressions();

                if (detections && detections.length > 0) {
                    // Get the dominant expression
                    const expressions = detections[0].expressions;
                    const dominantExpression = Object.keys(expressions).reduce((a, b) => expressions[a] > expressions[b] ? a : b);
                    currentExpression = dominantExpression;
                    analyzeButton.disabled = false;
                    personaButton.disabled = false;
                } else {
                    currentExpression = null;
                    analyzeButton.disabled = true;
                    personaButton.disabled = true;
                }

                if (!videoContainer.contains(canvas)) {
                     clearInterval(detectionInterval);
                     return;
                }

                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                faceapi.draw.drawDetections(canvas, resizedDetections);
                faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
                faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
            }, 200); // Slower interval to not overload the API and browser
        });

        loadModels();
    </script>
</body>
</html>

